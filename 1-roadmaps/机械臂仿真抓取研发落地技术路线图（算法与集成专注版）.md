# 机械臂仿真抓取研发落地技术路线图（算法与集成专注版）

**文档版本：** v1.0  
**创建日期：** 2025-10-09  
**适用场景：** 3C制造领域机械臂抓取系统研发  
**技术特点：** 开源技术栈 + 开箱即用资源 + 算法聚焦 + 可扩展架构

---

## 📋 文档目标

本路线图旨在为机械臂抓取研发团队提供：

1. **开箱即用的仿真资源** - 3C零件模型 + 机械臂URDF + 预配置场景
2. **系统化算法研发流程** - 从感知到控制的完整pipeline
3. **高效的仿真集成方案** - 端到端工作流 + 调试工具链
4. **开源可扩展架构** - 模块化设计 + 支持后续功能扩展

**核心理念：** 让团队聚焦于算法创新和系统优化，而非基础环境搭建。

---

## 🎯 第一部分：开箱即用的仿真资源清单

### 1.1 机械臂仿真资源（Ready-to-Use）

#### 1.1.1 推荐机械臂型号及URDF资源

| 机械臂型号 | 开源URDF仓库 | 特点 | 适用场景 |
|-----------|-------------|------|---------|
| **Universal Robots UR5/UR5e** | `github.com/ros-industrial/universal_robot`<br>`github.com/UniversalRobots/Universal_Robots_ROS2_Description` | • 工业标准<br>• 重复精度±0.03mm<br>• 社区支持最好 | 高精度3C装配<br>PCB操作 |
| **Franka Emika Panda** | `github.com/frankaemika/franka_ros`<br>`github.com/frankaemika/franka_ros2` | • 7自由度<br>• 力反馈优秀<br>• 学术界首选 | 柔性抓取<br>接触力敏感任务 |
| **AUBO i5** | `github.com/aubo-robot/aubo_robot` | • 国产方案<br>• 性价比高<br>• 适合中小团队 | 一般精度抓取<br>教育研发 |
| **Kinova Gen3** | `github.com/Kinovarobotics/ros_kortex` | • 7自由度<br>• 灵活性高<br>• 适合复杂姿态 | 狭小空间抓取<br>多角度操作 |

**快速启动命令：**

```bash
# UR5 (ROS 2)
git clone https://github.com/UniversalRobots/Universal_Robots_ROS2_Description.git
cd Universal_Robots_ROS2_Description
colcon build

# Franka Panda (ROS 2)
git clone https://github.com/frankaemika/franka_ros2.git
cd franka_ros2
colcon build --cmake-args -DCMAKE_BUILD_TYPE=Release
```

#### 1.1.2 末端执行器（夹爪）资源

| 夹爪型号 | URDF/模型资源 | 特点 |
|---------|--------------|------|
| **Robotiq 2F-85** | `github.com/JStech/robotiq_2f_description` | 标准平行夹爪，适合规则零件 |
| **Robotiq Hand-E** | `github.com/cambel/robotiq` | 紧凑型，适合狭小空间 |
| **Franka Hand** | 包含在`franka_ros`中 | 力反馈优秀，适合柔性抓取 |
| **吸盘夹爪（自定义）** | 需自行建模（见1.3节） | 适合平面3C零件（PCB、屏幕） |

### 1.2 3C零件仿真模型资源

#### 1.2.1 现成物体数据集（可直接使用）

| 数据集名称 | 规模 | 下载地址 | 格式 | 3C相关性 |
|-----------|------|---------|------|---------|
| **YCB Objects** | 77个日常物体 | `ycb-benchmarks.s3-website-us-east-1.amazonaws.com` | OBJ, STL, URDF | ⭐⭐☆ (部分电子产品) |
| **GraspNet-1B Objects** | 88个物体 | `graspnet.net/datasets.html` | OBJ, PLY | ⭐⭐⭐ (包含电子零件) |
| **DexGraspNet** | 5355个物体 | `pku-epic.github.io/DexGraspNet` | URDF, OBJ | ⭐⭐⭐⭐ (大量小零件) |
| **Google Scanned Objects** | 1032个物体 | `fuel.gazebosim.org/GoogleResearch/GSODataset` | OBJ, SDF | ⭐⭐⭐ (多样性高) |
| **OmniObject3D** | 6000+物体 | `omniobject3d.github.io` | OBJ, USD | ⭐⭐⭐⭐ (精度高) |

**快速下载脚本：**

```bash
# YCB Objects (示例：下载前10个)
mkdir -p ~/grasp_resources/ycb_objects
cd ~/grasp_resources/ycb_objects
for i in {001..010}; do
    wget http://ycb-benchmarks.s3-website-us-east-1.amazonaws.com/data/objects/${i}_*/google_16k/textured.obj
done

# GraspNet-1B Objects (需先注册)
# 访问 https://graspnet.net 注册后获取下载链接

# Google Scanned Objects (通过Fuel下载)
ign fuel download -u https://fuel.gazebosim.org/1.0/GoogleResearch/models/Google_Scanned_Objects
```

#### 1.2.2 专用3C零件模型库

由于专用3C零件模型较少，推荐以下获取方式：

**方案A：开源CAD库**

| 资源网站 | 内容 | 访问方式 |
|---------|------|---------|
| **GrabCAD** | SMT元件、PCB板、连接器 | `grabcad.com/library` (免费注册) |
| **Thingiverse** | 电子零件3D打印模型 | `thingiverse.com` (搜索"SMT", "PCB") |
| **TraceParts** | 工业标准零件库 | `traceparts.com` (需企业账号) |

**方案B：自动生成参数化模型（推荐）**

使用OpenSCAD或FreeCAD生成标准3C零件：

```python
# 示例：生成0402 SMT芯片模型（Python + trimesh）
import trimesh
import numpy as np

def generate_smt_0402():
    """生成0402贴片电阻模型 (1.0mm x 0.5mm x 0.35mm)"""
    # 尺寸（单位：米）
    length, width, height = 0.001, 0.0005, 0.00035
    
    # 创建box mesh
    box = trimesh.creation.box(extents=[length, width, height])
    
    # 设置物理属性
    box.density = 1600  # kg/m³ (陶瓷密度)
    
    # 导出为URDF
    box.export('smt_0402.obj')
    
    # 生成URDF文件
    urdf_template = f"""
    <robot name="smt_0402">
      <link name="base_link">
        <visual>
          <geometry>
            <mesh filename="smt_0402.obj" scale="1 1 1"/>
          </geometry>
          <material name="black">
            <color rgba="0.1 0.1 0.1 1"/>
          </material>
        </visual>
        <collision>
          <geometry>
            <box size="{length} {width} {height}"/>
          </geometry>
        </collision>
        <inertial>
          <mass value="{box.mass}"/>
          <inertia ixx="1e-8" ixy="0" ixz="0" iyy="1e-8" iyz="0" izz="1e-8"/>
        </inertial>
      </link>
    </robot>
    """
    
    with open('smt_0402.urdf', 'w') as f:
        f.write(urdf_template)
    
    return box

# 批量生成常见SMT尺寸
smt_sizes = {
    '0201': (0.0006, 0.0003, 0.00026),
    '0402': (0.001, 0.0005, 0.00035),
    '0603': (0.0016, 0.0008, 0.00045),
    '0805': (0.002, 0.00125, 0.0006),
    '1206': (0.0032, 0.0016, 0.0006)
}

for name, dims in smt_sizes.items():
    print(f"Generating {name}...")
    # 调整尺寸参数生成
```

**方案C：3D扫描现有零件**

工具推荐：
- **Intel RealSense D435i** + **Open3D** - 低成本扫描方案
- **Artec Eva** - 专业级扫描仪（高精度）

```python
# Open3D扫描后处理示例
import open3d as o3d

# 读取点云
pcd = o3d.io.read_point_cloud("scanned_component.ply")

# 降噪
pcd = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)[0]

# 网格重建
mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)

# 简化mesh（减少三角形数量）
mesh = mesh.simplify_quadric_decimation(target_number_of_triangles=5000)

# 导出
o3d.io.write_triangle_mesh("component.obj", mesh)
```

### 1.3 预配置仿真场景

#### 1.3.1 Isaac Lab场景（推荐）

**Isaac Lab** 是NVIDIA开源的轻量级仿真框架，基于Isaac Sim构建。

**优势：**
- ✅ GPU加速物理仿真（16K+ 并行环境）
- ✅ RTX光线追踪（高精度视觉传感器）
- ✅ 内置强化学习接口
- ✅ 完全免费开源

**安装：**

```bash
# 1. 安装Isaac Sim (需NVIDIA GPU)
# 下载：https://developer.nvidia.com/isaac-sim
# 或使用Docker：
docker pull nvcr.io/nvidia/isaac-sim:2023.1.1

# 2. 安装Isaac Lab
git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab
./isaaclab.sh --install

# 3. 测试安装
./isaaclab.sh -p source/standalone/tutorials/00_sim/create_empty.py
```

**预配置抓取场景：**

Isaac Lab提供的现成场景：

| 场景名称 | 路径 | 描述 |
|---------|------|------|
| **Franka抓取场景** | `source/standalone/environments/manipulation/reach` | Franka机械臂到达目标点 |
| **物体抓取场景** | `source/standalone/environments/manipulation/lift` | 抓取并提升物体 |
| **Bin Picking** | `source/extensions/omni.isaac.lab_tasks/manipulation/inhand` | 料箱拣选场景 |

**自定义3C场景示例：**

```python
# 创建3C零件抓取场景
# 保存为：IsaacLab/source/standalone/custom/smt_grasping.py

import torch
from omni.isaac.lab.app import AppLauncher

# 启动仿真
app_launcher = AppLauncher(headless=False)
simulation_app = app_launcher.app

from omni.isaac.lab.assets import Articulation, RigidObject
from omni.isaac.lab.sim import SimulationContext
import omni.isaac.lab.sim as sim_utils

# 创建场景
class SMTGraspingScene:
    def __init__(self):
        # 1. 加载机械臂
        self.robot = Articulation(
            prim_path="/World/Robot",
            spawn=sim_utils.UsdFileCfg(
                usd_path="path/to/ur5e.usd",  # 使用Isaac Sim资产库
                rigid_props=sim_utils.RigidBodyPropertiesCfg(),
            ),
        )
        
        # 2. 加载工作台
        self.table = RigidObject(
            prim_path="/World/Table",
            spawn=sim_utils.CuboidCfg(
                size=(0.8, 0.8, 0.05),
                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.5, 0.5, 0.5)),
            ),
        )
        
        # 3. 批量生成SMT零件（支持GPU并行）
        self.num_components = 100
        self.components = RigidObject(
            prim_path="/World/Components/Component",
            spawn=sim_utils.UsdFileCfg(
                usd_path="path/to/smt_0402.usd",
                rigid_props=sim_utils.RigidBodyPropertiesCfg(
                    rigid_body_enabled=True,
                    mass=0.0001,  # 0.1g
                ),
            ),
            init_state=RigidObject.InitialStateCfg(
                pos=self._generate_random_positions(self.num_components),
            ),
        )
        
        # 4. 添加RGB-D相机
        self.camera = sim_utils.CameraCfg(
            prim_path="/World/Camera",
            update_period=0.1,
            height=480,
            width=640,
            data_types=["rgb", "distance_to_camera", "normals"],
        )
    
    def _generate_random_positions(self, n):
        """在工作台上生成随机位置"""
        x = torch.rand(n) * 0.6 - 0.3  # [-0.3, 0.3]
        y = torch.rand(n) * 0.6 - 0.3
        z = torch.ones(n) * 0.05  # 工作台上方
        return torch.stack([x, y, z], dim=-1)

# 运行场景
if __name__ == "__main__":
    scene = SMTGraspingScene()
    print("3C抓取场景已加载！")
```

#### 1.3.2 MuJoCo场景（轻量级方案）

**MuJoCo** 是DeepMind开源的物理引擎，适合快速算法原型。

**优势：**
- ✅ 速度极快（CPU优化）
- ✅ 接触力学精确
- ✅ 支持MJX（JAX加速版，1000x并行）

**安装：**

```bash
pip install mujoco
pip install mujoco-mjx  # GPU加速版
```

**预配置场景XML：**

```xml
<!-- 保存为：ur5_smt_grasping.xml -->
<mujoco model="UR5 SMT Grasping">
  <compiler angle="radian" meshdir="meshes/"/>
  
  <asset>
    <!-- 加载UR5模型 -->
    <mesh name="base" file="ur5/base.stl"/>
    <mesh name="shoulder" file="ur5/shoulder.stl"/>
    <mesh name="upperarm" file="ur5/upperarm.stl"/>
    <mesh name="forearm" file="ur5/forearm.stl"/>
    <mesh name="wrist1" file="ur5/wrist1.stl"/>
    <mesh name="wrist2" file="ur5/wrist2.stl"/>
    <mesh name="wrist3" file="ur5/wrist3.stl"/>
    
    <!-- SMT零件 -->
    <mesh name="smt_0402" file="smt_0402.obj" scale="1 1 1"/>
  </asset>
  
  <worldbody>
    <!-- 地面 -->
    <geom name="floor" type="plane" size="2 2 0.1" rgba="0.8 0.8 0.8 1"/>
    
    <!-- 工作台 -->
    <body name="table" pos="0 0 0">
      <geom name="table_top" type="box" size="0.4 0.4 0.025" rgba="0.5 0.5 0.5 1"/>
    </body>
    
    <!-- UR5机械臂 -->
    <body name="ur5_base" pos="0 0 0.025">
      <geom name="base_geom" type="mesh" mesh="base" rgba="0.7 0.7 0.7 1"/>
      <joint name="shoulder_pan" type="hinge" axis="0 0 1" range="-3.14159 3.14159" damping="1"/>
      
      <!-- ... 其他关节 ... -->
    </body>
    
    <!-- SMT零件（可批量生成） -->
    <body name="smt_001" pos="0.1 0.1 0.05">
      <geom name="smt_geom_001" type="mesh" mesh="smt_0402" rgba="0.1 0.1 0.1 1" mass="0.0001"/>
      <joint name="smt_free_001" type="free"/>
    </body>
    
    <!-- 相机（模拟RGB-D） -->
    <camera name="workspace_cam" pos="0 -0.5 0.5" xyaxes="1 0 0 0 0.7 0.7"/>
  </worldbody>
  
  <actuator>
    <motor name="shoulder_pan_motor" joint="shoulder_pan" ctrlrange="-2 2"/>
    <!-- ... 其他电机 ... -->
  </actuator>
</mujoco>
```

**加载场景：**

```python
import mujoco
import mujoco.viewer

# 加载模型
model = mujoco.MjModel.from_xml_path('ur5_smt_grasping.xml')
data = mujoco.MjData(model)

# 启动可视化
with mujoco.viewer.launch_passive(model, data) as viewer:
    while viewer.is_running():
        mujoco.mj_step(model, data)
        viewer.sync()
```

#### 1.3.3 Gazebo场景（ROS集成最佳）

**Gazebo** 适合ROS生态系统集成。

**预配置场景包：**

```bash
# 安装Gazebo + ROS 2
sudo apt install ros-humble-gazebo-ros-pkgs

# 克隆示例场景
git clone https://github.com/ros-industrial/easy_manipulation_deployment.git
cd easy_manipulation_deployment
colcon build

# 启动3C抓取场景
ros2 launch emd_grasp_demo grasp_demo.launch.py
```

### 1.4 视觉传感器仿真

#### 1.4.1 RGB-D相机配置

| 真实硬件 | 仿真等效配置 | Isaac Sim | Gazebo |
|---------|------------|----------|--------|
| Intel RealSense D435i | 深度范围: 0.3-3m<br>分辨率: 1280x720 | ✅ 内置支持 | ✅ `gazebo_ros_camera` |
| Azure Kinect | 深度范围: 0.5-5m<br>分辨率: 1024x1024 | ✅ 自定义配置 | ✅ 插件支持 |
| Zivid 2+ | 高精度点云<br>精度: ±0.02mm | ✅ 高级光追 | ⚠️ 需手动配置 |

**Isaac Sim相机配置：**

```python
from omni.isaac.sensor import Camera

camera_cfg = Camera.CameraCfg(
    prim_path="/World/Camera",
    update_period=0.1,  # 10Hz
    height=720,
    width=1280,
    data_types=["rgb", "distance_to_camera", "instance_segmentation"],
    spawn=sim_utils.PinholeCameraCfg(
        focal_length=1.88,  # RealSense D435i等效
        focus_distance=400.0,
        f_stop=1.8,
        horizontal_aperture=4.8,
        clipping_range=(0.01, 1000.0),
    ),
)
```

**Gazebo相机插件：**

```xml
<gazebo reference="camera_link">
  <sensor name="rgbd_camera" type="depth">
    <update_rate>30</update_rate>
    <camera>
      <horizontal_fov>1.047</horizontal_fov>
      <image>
        <width>1280</width>
        <height>720</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.3</near>
        <far>3.0</far>
      </clip>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <ros>
        <namespace>/camera</namespace>
        <remapping>image_raw:=rgb/image_raw</remapping>
        <remapping>depth/image_raw:=depth/image_raw</remapping>
      </ros>
    </plugin>
  </sensor>
</gazebo>
```

---

## 🧠 第二部分：算法研发完整流程

### 2.1 感知层算法

#### 2.1.1 物体检测与分割

**技术栈选择：**

| 任务类型 | 推荐算法 | 开源实现 | 速度/精度 |
|---------|---------|---------|----------|
| 2D目标检测 | YOLOv8/v9 | `ultralytics/ultralytics` | ⚡⚡⚡ 30ms/帧 |
| 实例分割 | Mask R-CNN | `matterport/Mask_RCNN` | ⚡⚡☆ 80ms/帧 |
| 3D点云分割 | PointNet++ | `yanx27/Pointnet_Pointnet2_pytorch` | ⚡⚡☆ 50ms/点云 |
| 语义分割 | Segment Anything (SAM) | `facebookresearch/segment-anything` | ⚡⚡☆ 100ms/帧 |

**实现步骤 - 以YOLOv8为例：**

```bash
# 1. 安装
pip install ultralytics

# 2. 准备数据集
# 数据集结构：
# dataset/
#   ├── images/
#   │   ├── train/
#   │   └── val/
#   └── labels/
#       ├── train/
#       └── val/
```

```python
# 3. 训练3C零件检测模型
from ultralytics import YOLO

# 加载预训练模型
model = YOLO('yolov8n.pt')  # nano版本，适合边缘部署

# 训练
results = model.train(
    data='3c_components.yaml',  # 数据集配置
    epochs=100,
    imgsz=640,
    batch=16,
    device=0,  # GPU
    project='3c_detection',
    name='smt_detector_v1',
    # 针对小物体优化
    mosaic=0.5,
    mixup=0.1,
    copy_paste=0.1,
)

# 4. 导出为ONNX（加速推理）
model.export(format='onnx', dynamic=True, simplify=True)
```

**数据集配置文件（3c_components.yaml）：**

```yaml
# 数据路径
path: ./dataset
train: images/train
val: images/val

# 类别
nc: 10  # 类别数量
names:
  0: smt_0402
  1: smt_0603
  2: smt_0805
  3: connector_usb
  4: connector_hdmi
  5: ic_chip_qfn
  6: ic_chip_bga
  7: capacitor_elect
  8: pcb_board
  9: flex_cable
```

#### 2.1.2 6D位姿估计

**推荐方法：**

| 方法 | 论文/代码 | 精度 | 速度 | 适用场景 |
|------|----------|------|------|---------|
| **PoseCNN** | `yuxng/PoseCNN-PyTorch` | ±5mm | 20Hz | 通用物体 |
| **DenseFusion** | `j96w/DenseFusion` | ±3mm | 12Hz | 精确抓取 |
| **GDR-Net** | `THU-DA-6D-Pose-Group/GDR-Net` | ±2mm | 15Hz | 高精度需求 |
| **FoundationPose** | `NVlabs/FoundationPose` | ±1mm | 10Hz | 少样本/新物体 |

**实现示例 - FoundationPose（CVPR 2024）：**

```bash
# 安装
git clone https://github.com/NVlabs/FoundationPose.git
cd FoundationPose
bash build_all.sh

# 下载预训练权重
wget https://huggingface.co/wenbowen/foundationpose/resolve/main/model.pth
```

```python
# 运行位姿估计
import numpy as np
import torch
from foundationpose.estimater import FoundationPose

# 初始化
estimater = FoundationPose(
    model_path='model.pth',
    mesh_path='path/to/smt_0402.obj',  # 物体3D模型
    debug=2,
)

# 读取RGB-D图像
color = cv2.imread('rgb.png')
depth = np.load('depth.npy')

# 估计位姿
pose = estimater.register(
    K=camera_intrinsics,  # 3x3相机内参矩阵
    rgb=color,
    depth=depth,
    ob_mask=object_mask,  # 物体分割mask
)

# 输出：4x4变换矩阵
print(f"物体位姿：\n{pose}")
# [[R11, R12, R13, tx],
#  [R21, R22, R23, ty],
#  [R31, R32, R33, tz],
#  [0,   0,   0,   1 ]]
```

#### 2.1.3 点云处理Pipeline

```python
import open3d as o3d
import numpy as np

class PointCloudProcessor:
    """3C零件点云处理pipeline"""
    
    def __init__(self):
        self.voxel_size = 0.001  # 1mm体素
    
    def process(self, rgbd_image, camera_intrinsic):
        """完整处理流程"""
        # 1. RGB-D转点云
        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
            rgbd_image,
            camera_intrinsic,
        )
        
        # 2. 降噪
        pcd = self.remove_noise(pcd)
        
        # 3. 平面分割（去除桌面）
        pcd = self.remove_plane(pcd)
        
        # 4. 聚类（分离多个物体）
        clusters = self.cluster_objects(pcd)
        
        return clusters
    
    def remove_noise(self, pcd):
        """统计滤波去噪"""
        pcd, _ = pcd.remove_statistical_outlier(
            nb_neighbors=20,
            std_ratio=2.0,
        )
        return pcd
    
    def remove_plane(self, pcd):
        """RANSAC平面分割"""
        plane_model, inliers = pcd.segment_plane(
            distance_threshold=0.005,  # 5mm
            ransac_n=3,
            num_iterations=1000,
        )
        # 返回非平面点（物体）
        object_pcd = pcd.select_by_index(inliers, invert=True)
        return object_pcd
    
    def cluster_objects(self, pcd):
        """DBSCAN聚类"""
        labels = np.array(pcd.cluster_dbscan(
            eps=0.01,  # 10mm
            min_points=10,
        ))
        
        # 分离每个cluster
        max_label = labels.max()
        clusters = []
        for i in range(max_label + 1):
            cluster_indices = np.where(labels == i)[0]
            cluster_pcd = pcd.select_by_index(cluster_indices)
            clusters.append(cluster_pcd)
        
        return clusters
```

### 2.2 抓取规划算法

#### 2.2.1 基于学习的抓取生成

**推荐方法：**

| 方法 | 代码仓库 | 特点 | 性能指标 |
|------|---------|------|---------|
| **GraspNet-1B** | `graspnet/graspnet-baseline` | 工业标准baseline | 82.3% AP@0.8 |
| **AnyGrasp** | `graspnet/anygrasp_sdk` | SOTA性能 | 92.7% 成功率 @ 0.03s |
| **Contact-GraspNet** | `NVlabs/contact_graspnet` | 精确接触点 | 91.2% 成功率 |
| **GraspNeRF** | `Ray3D/GraspNeRF` | 处理透明物体 | 88% on transparent |

**实现 - AnyGrasp（推荐用于生产）：**

```bash
# 1. 安装
pip install anygrasp
# 需申请license：https://graspnet.net/anygrasp.html
```

```python
# 2. 使用AnyGrasp生成抓取
from anygrasp import AnyGraspDetector
import numpy as np

# 初始化检测器
detector = AnyGraspDetector(
    cfgs_path='config.yaml',
    checkpoint_path='checkpoint.tar',
)

# 输入点云（Nx3数组）
point_cloud = np.load('scene_pointcloud.npy')

# 生成抓取
grasps, scores = detector.predict(point_cloud)

# 输出格式：
# grasps: (N, 4, 4) - N个抓取的4x4变换矩阵
# scores: (N,) - 每个抓取的置信度分数

# 选择最佳抓取
best_grasp_idx = np.argmax(scores)
best_grasp = grasps[best_grasp_idx]

print(f"最佳抓取位姿：\n{best_grasp}")
print(f"置信度：{scores[best_grasp_idx]:.2f}")
```

**从头训练自定义抓取网络：**

```python
# 基于GraspNet-1B训练自定义模型
import torch
from graspnetAPI import GraspNet

# 1. 加载GraspNet数据集
dataset = GraspNet(
    root='/path/to/graspnet',
    camera='realsense',
    split='train',
)

# 2. 定义训练循环
from graspnet_baseline import GraspNetModel

model = GraspNetModel(
    num_view=300,
    num_angle=12,
    num_depth=4,
    cylinder_radius=0.05,
    hmin=-0.02,
    hmax=0.08,
)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(100):
    for batch in dataset:
        points = batch['point_clouds']  # (B, N, 3)
        grasp_labels = batch['grasp_labels']  # (B, ...)
        
        # 前向传播
        pred_grasps, pred_scores = model(points)
        
        # 计算损失
        loss = compute_grasp_loss(pred_grasps, pred_scores, grasp_labels)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
```

#### 2.2.2 抓取质量评估

```python
class GraspQualityEvaluator:
    """抓取质量评估器"""
    
    def evaluate(self, grasp_pose, object_mesh):
        """
        评估抓取质量
        
        Args:
            grasp_pose: 4x4抓取位姿矩阵
            object_mesh: trimesh.Trimesh物体模型
        
        Returns:
            quality_score: 0-1之间的质量分数
        """
        scores = {}
        
        # 1. 力闭包检查
        scores['force_closure'] = self.check_force_closure(grasp_pose, object_mesh)
        
        # 2. 防碰撞检查
        scores['collision_free'] = self.check_collision(grasp_pose, object_mesh)
        
        # 3. 抓取稳定性
        scores['stability'] = self.compute_stability(grasp_pose, object_mesh)
        
        # 4. 可达性
        scores['reachability'] = self.check_reachability(grasp_pose)
        
        # 加权求和
        weights = {
            'force_closure': 0.4,
            'collision_free': 0.3,
            'stability': 0.2,
            'reachability': 0.1,
        }
        
        total_score = sum(scores[k] * weights[k] for k in weights)
        return total_score, scores
    
    def check_force_closure(self, grasp_pose, object_mesh):
        """检查力闭包（简化版）"""
        # 获取接触点
        contact_points = self.get_contact_points(grasp_pose, object_mesh)
        
        if len(contact_points) < 2:
            return 0.0
        
        # 计算接触点法向量
        normals = self.compute_contact_normals(contact_points, object_mesh)
        
        # 检查法向量是否相对
        angle = np.arccos(np.dot(normals[0], -normals[1]))
        
        # 接近180度得分高
        score = 1.0 - abs(angle - np.pi) / np.pi
        return score
    
    def check_collision(self, grasp_pose, object_mesh):
        """碰撞检测"""
        import trimesh
        
        # 加载夹爪模型
        gripper_mesh = trimesh.load('gripper.obj')
        
        # 变换到抓取位姿
        gripper_mesh.apply_transform(grasp_pose)
        
        # 检查碰撞
        collision_manager = trimesh.collision.CollisionManager()
        collision_manager.add_object('gripper', gripper_mesh)
        collision_manager.add_object('object', object_mesh)
        
        is_collision = collision_manager.in_collision_internal()
        
        return 0.0 if is_collision else 1.0
```

### 2.3 运动规划算法

#### 2.3.1 MoveIt 2配置

```bash
# 安装MoveIt 2
sudo apt install ros-humble-moveit

# 创建MoveIt配置（以UR5为例）
ros2 launch moveit_setup_assistant setup_assistant.launch.py
```

**Python接口使用：**

```python
import rclpy
from moveit_msgs.msg import MoveItErrorCodes
from moveit_py import MoveItPy

class GraspPlanner:
    def __init__(self):
        rclpy.init()
        self.moveit = MoveItPy(node_name="grasp_planner")
        
        self.arm = self.moveit.get_planning_component("manipulator")
        self.gripper = self.moveit.get_planning_component("gripper")
    
    def plan_grasp(self, target_pose):
        """规划抓取运动"""
        # 1. 移动到预抓取位置（目标上方10cm）
        pre_grasp_pose = target_pose.copy()
        pre_grasp_pose[2, 3] += 0.1  # Z轴上移10cm
        
        self.arm.set_goal_state(pose=pre_grasp_pose)
        plan1 = self.arm.plan()
        
        if plan1.error_code != MoveItErrorCodes.SUCCESS:
            print("预抓取规划失败！")
            return None
        
        # 2. 打开夹爪
        self.gripper.set_named_target("open")
        plan2 = self.gripper.plan()
        
        # 3. 下降到抓取位置
        self.arm.set_goal_state(pose=target_pose)
        plan3 = self.arm.plan()
        
        # 4. 闭合夹爪
        self.gripper.set_named_target("close")
        plan4 = self.gripper.plan()
        
        # 5. 提升物体
        lift_pose = target_pose.copy()
        lift_pose[2, 3] += 0.15
        self.arm.set_goal_state(pose=lift_pose)
        plan5 = self.arm.plan()
        
        # 6. 执行所有步骤
        plans = [plan1, plan2, plan3, plan4, plan5]
        for i, plan in enumerate(plans):
            print(f"执行步骤 {i+1}/5...")
            self.arm.execute(plan)
        
        return True
```

#### 2.3.2 碰撞避障

```python
from moveit_msgs.msg import PlanningScene, CollisionObject
from shape_msgs.msg import SolidPrimitive
from geometry_msgs.msg import Pose

class CollisionSceneManager:
    """管理碰撞场景"""
    
    def __init__(self, moveit_py):
        self.moveit = moveit_py
        self.planning_scene = self.moveit.get_planning_scene()
    
    def add_table(self, height=0.025):
        """添加工作台"""
        table = CollisionObject()
        table.id = "table"
        table.header.frame_id = "world"
        
        # 定义box
        box = SolidPrimitive()
        box.type = SolidPrimitive.BOX
        box.dimensions = [0.8, 0.8, height]
        
        # 位置
        pose = Pose()
        pose.position.z = height / 2
        pose.orientation.w = 1.0
        
        table.primitives.append(box)
        table.primitive_poses.append(pose)
        
        # 添加到场景
        self.planning_scene.add_object(table)
    
    def add_objects_from_perception(self, detected_objects):
        """从感知结果添加物体"""
        for i, obj in enumerate(detected_objects):
            collision_obj = CollisionObject()
            collision_obj.id = f"object_{i}"
            collision_obj.header.frame_id = "world"
            
            # 使用mesh
            collision_obj.meshes.append(obj.mesh)
            collision_obj.mesh_poses.append(obj.pose)
            
            self.planning_scene.add_object(collision_obj)
```

### 2.4 强化学习训练（高级）

#### 2.4.1 Isaac Lab + RL框架

```python
# 使用Isaac Lab训练抓取策略
from omni.isaac.lab_tasks.manipulation.lift import LiftEnvCfg
from omni.isaac.lab.envs import ManagerBasedRLEnv
from stable_baselines3 import PPO

# 1. 配置环境
env_cfg = LiftEnvCfg()
env_cfg.scene.num_envs = 4096  # GPU并行
env = ManagerBasedRLEnv(cfg=env_cfg)

# 2. 配置PPO算法
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=1024,
    batch_size=512,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    verbose=1,
    tensorboard_log="./ppo_grasp_logs/",
)

# 3. 训练
model.learn(
    total_timesteps=10_000_000,
    callback=checkpoint_callback,
)

# 4. 保存模型
model.save("ppo_grasp_policy")

# 5. 测试
obs, info = env.reset()
for _ in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    
    if terminated or truncated:
        obs, info = env.reset()
```

#### 2.4.2 Domain Randomization配置

```python
# Isaac Lab中的Domain Randomization
from omni.isaac.lab.utils import configclass
from omni.isaac.lab.managers import EventTermCfg as EventTerm

@configclass
class DomainRandomizationCfg:
    """领域随机化配置"""
    
    # 1. 物理参数随机化
    randomize_object_mass = EventTerm(
        func=randomize_mass,
        params={"mass_range": (0.05, 0.5)},  # kg
        mode="startup",
    )
    
    randomize_friction = EventTerm(
        func=randomize_friction,
        params={"friction_range": (0.5, 1.5)},
        mode="startup",
    )
    
    # 2. 视觉随机化
    randomize_lighting = EventTerm(
        func=randomize_lights,
        params={
            "intensity_range": (500, 5000),  # lumens
            "color_temp_range": (2000, 6500),  # Kelvin
        },
        mode="interval",
        interval_range_s=(5.0, 10.0),
    )
    
    randomize_textures = EventTerm(
        func=randomize_textures,
        params={"texture_library": "textures/"},
        mode="reset",
    )
    
    # 3. 几何随机化
    randomize_object_scale = EventTerm(
        func=randomize_scale,
        params={"scale_range": (0.9, 1.1)},
        mode="reset",
    )
    
    randomize_object_pose = EventTerm(
        func=randomize_pose,
        params={
            "position_range": [(-0.3, 0.3), (-0.3, 0.3), (0.0, 0.1)],
            "orientation_range": [(0, 2*np.pi), (0, 0), (0, 0)],
        },
        mode="reset",
    )
```

---

## 🔧 第三部分：仿真集成环节

### 3.1 端到端仿真Pipeline

```python
class GraspingSimulationPipeline:
    """完整抓取仿真Pipeline"""
    
    def __init__(self, sim_platform='isaac_lab'):
        # 初始化各模块
        self.perception = PerceptionModule()
        self.grasp_planner = GraspPlanningModule()
        self.motion_planner = MotionPlanningModule()
        self.controller = RobotController()
        
        # 初始化仿真环境
        if sim_platform == 'isaac_lab':
            self.sim = IsaacLabSimulation()
        elif sim_platform == 'mujoco':
            self.sim = MuJoCoSimulation()
        else:
            raise ValueError(f"Unsupported platform: {sim_platform}")
    
    def run_episode(self):
        """运行一个完整抓取episode"""
        # 1. 重置环境
        obs = self.sim.reset()
        rgb_image = obs['camera']['rgb']
        depth_image = obs['camera']['depth']
        
        # 2. 感知：检测物体
        detected_objects = self.perception.detect_objects(rgb_image, depth_image)
        
        if len(detected_objects) == 0:
            print("未检测到物体！")
            return False
        
        # 3. 选择目标物体（最近的）
        target_object = self.select_target(detected_objects)
        
        # 4. 估计6D位姿
        object_pose = self.perception.estimate_pose(
            rgb_image, depth_image, target_object
        )
        
        # 5. 生成抓取
        grasps, scores = self.grasp_planner.generate_grasps(
            object_pose, target_object.model
        )
        best_grasp = grasps[np.argmax(scores)]
        
        # 6. 规划运动
        trajectory = self.motion_planner.plan_grasp_motion(
            current_pose=self.sim.get_robot_state(),
            grasp_pose=best_grasp,
        )
        
        if trajectory is None:
            print("运动规划失败！")
            return False
        
        # 7. 执行轨迹
        success = self.execute_trajectory(trajectory)
        
        # 8. 评估结果
        is_grasped = self.check_grasp_success()
        
        return is_grasped
    
    def execute_trajectory(self, trajectory):
        """执行轨迹"""
        for waypoint in trajectory:
            # 计算关节控制指令
            joint_targets = self.controller.compute_joint_targets(waypoint)
            
            # 发送到仿真器
            self.sim.set_joint_targets(joint_targets)
            
            # 步进仿真
            for _ in range(10):  # 每个waypoint仿真10步
                self.sim.step()
        
        return True
    
    def check_grasp_success(self):
        """检查抓取是否成功"""
        # 提升夹爪
        self.sim.lift_gripper(height=0.1)
        
        # 检查物体是否跟随
        object_height = self.sim.get_object_height()
        
        return object_height > 0.05  # 物体抬起5cm以上
```

### 3.2 数据采集与记录

#### 3.2.1 ROS 2 Bag录制

```python
import rclpy
from rclpy.node import Node
from rosbag2_py import SequentialWriter, StorageOptions, ConverterOptions

class DataRecorder(Node):
    """ROS 2数据记录器"""
    
    def __init__(self):
        super().__init__('data_recorder')
        
        # 配置bag writer
        storage_options = StorageOptions(
            uri='grasp_data',
            storage_id='sqlite3',
        )
        converter_options = ConverterOptions(
            input_serialization_format='cdr',
            output_serialization_format='cdr',
        )
        
        self.writer = SequentialWriter()
        self.writer.open(storage_options, converter_options)
        
        # 订阅话题
        self.create_subscription(
            Image, '/camera/rgb/image_raw', self.rgb_callback, 10
        )
        self.create_subscription(
            Image, '/camera/depth/image_raw', self.depth_callback, 10
        )
        self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10
        )
    
    def rgb_callback(self, msg):
        self.writer.write('/camera/rgb/image_raw', serialize_message(msg), self.get_clock().now().nanoseconds)
    
    # ... 其他回调函数
```

#### 3.2.2 自定义数据格式

```python
import h5py
import numpy as np

class GraspDataset:
    """自定义抓取数据集格式"""
    
    def __init__(self, filename):
        self.file = h5py.File(filename, 'w')
        
        # 创建数据组
        self.file.create_group('episodes')
        self.current_episode = 0
    
    def save_episode(self, episode_data):
        """
        保存一个episode的数据
        
        episode_data: {
            'rgb_images': (T, H, W, 3),
            'depth_images': (T, H, W),
            'joint_states': (T, 6),
            'gripper_states': (T, 1),
            'grasp_pose': (4, 4),
            'success': bool,
        }
        """
        ep_group = self.file['episodes'].create_group(f'ep_{self.current_episode}')
        
        for key, value in episode_data.items():
            ep_group.create_dataset(key, data=value, compression='gzip')
        
        self.current_episode += 1
    
    def close(self):
        self.file.close()

# 使用示例
dataset = GraspDataset('grasp_dataset.h5')

for i in range(1000):
    episode_data = run_grasp_episode()
    dataset.save_episode(episode_data)

dataset.close()
```

### 3.3 性能评估工具

```python
class GraspingBenchmark:
    """抓取系统性能评估"""
    
    def __init__(self):
        self.metrics = {
            'success_rate': [],
            'grasp_time': [],
            'planning_time': [],
            'execution_time': [],
            'collision_count': [],
        }
    
    def run_benchmark(self, pipeline, num_episodes=100):
        """运行benchmark"""
        print(f"运行{num_episodes}次抓取测试...")
        
        for i in range(num_episodes):
            start_time = time.time()
            
            # 运行抓取
            result = pipeline.run_episode()
            
            end_time = time.time()
            
            # 记录指标
            self.metrics['success_rate'].append(1.0 if result['success'] else 0.0)
            self.metrics['grasp_time'].append(end_time - start_time)
            self.metrics['planning_time'].append(result['planning_time'])
            self.metrics['execution_time'].append(result['execution_time'])
            self.metrics['collision_count'].append(result['collisions'])
            
            if (i + 1) % 10 == 0:
                print(f"进度: {i+1}/{num_episodes}")
        
        # 计算统计结果
        self.print_results()
    
    def print_results(self):
        """打印benchmark结果"""
        print("\n" + "="*50)
        print("Benchmark Results")
        print("="*50)
        
        print(f"成功率: {np.mean(self.metrics['success_rate']) * 100:.1f}%")
        print(f"平均抓取时间: {np.mean(self.metrics['grasp_time']):.2f}s")
        print(f"平均规划时间: {np.mean(self.metrics['planning_time']):.2f}s")
        print(f"平均执行时间: {np.mean(self.metrics['execution_time']):.2f}s")
        print(f"平均碰撞次数: {np.mean(self.metrics['collision_count']):.2f}")
        
        print("\n" + "="*50)
```

### 3.4 Sim2Real验证

```python
class Sim2RealTransfer:
    """仿真到真实环境迁移"""
    
    def __init__(self, sim_env, real_env):
        self.sim_env = sim_env
        self.real_env = real_env
    
    def validate_transfer(self, policy, num_trials=50):
        """验证Sim2Real迁移效果"""
        sim_results = []
        real_results = []
        
        # 1. 在仿真中测试
        print("在仿真环境中测试...")
        for i in range(num_trials):
            obs = self.sim_env.reset()
            success = self.run_policy(policy, obs, self.sim_env)
            sim_results.append(success)
        
        # 2. 在真实环境中测试
        print("在真实环境中测试...")
        for i in range(num_trials):
            obs = self.real_env.reset()
            success = self.run_policy(policy, obs, self.real_env)
            real_results.append(success)
        
        # 3. 分析迁移gap
        sim_success_rate = np.mean(sim_results)
        real_success_rate = np.mean(real_results)
        transfer_gap = sim_success_rate - real_success_rate
        
        print(f"\n仿真成功率: {sim_success_rate*100:.1f}%")
        print(f"真实成功率: {real_success_rate*100:.1f}%")
        print(f"迁移gap: {transfer_gap*100:.1f}%")
        
        # 4. 分析失败原因
        self.analyze_failures(real_results)
        
        return {
            'sim_success_rate': sim_success_rate,
            'real_success_rate': real_success_rate,
            'transfer_gap': transfer_gap,
        }
    
    def analyze_failures(self, results):
        """分析失败原因"""
        # TODO: 实现失败原因分类
        pass
```

---

## 🏗️ 第四部分：系统架构与扩展

### 4.1 模块化系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                   Grasping System Architecture               │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────┐      ┌──────────────┐      ┌──────────┐  │
│  │  Perception  │──────│  Planning    │──────│ Control  │  │
│  │   Module     │      │   Module     │      │  Module  │  │
│  └──────────────┘      └──────────────┘      └──────────┘  │
│         │                      │                    │       │
│         │                      │                    │       │
│         ▼                      ▼                    ▼       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │           Simulation / Real Robot Interface          │  │
│  └──────────────────────────────────────────────────────┘  │
│         │                      │                    │       │
│         ▼                      ▼                    ▼       │
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐│
│  │ Isaac Lab   │      │  MuJoCo     │      │Real Hardware││
│  └─────────────┘      └─────────────┘      └─────────────┘│
└─────────────────────────────────────────────────────────────┘
```

**接口定义：**

```python
from abc import ABC, abstractmethod

class PerceptionInterface(ABC):
    """感知模块接口"""
    
    @abstractmethod
    def detect_objects(self, rgb_image, depth_image):
        """检测物体"""
        pass
    
    @abstractmethod
    def estimate_pose(self, rgb_image, depth_image, object_mask):
        """估计6D位姿"""
        pass

class PlanningInterface(ABC):
    """规划模块接口"""
    
    @abstractmethod
    def generate_grasps(self, object_pointcloud):
        """生成抓取"""
        pass
    
    @abstractmethod
    def plan_motion(self, start_pose, goal_pose):
        """规划运动"""
        pass

class ControlInterface(ABC):
    """控制模块接口"""
    
    @abstractmethod
    def execute_trajectory(self, trajectory):
        """执行轨迹"""
        pass
    
    @abstractmethod
    def get_robot_state(self):
        """获取机器人状态"""
        pass
```

### 4.2 配置文件驱动

```yaml
# config/grasp_system.yaml
system:
  name: "3C Grasping System"
  version: "1.0"

simulation:
  platform: "isaac_lab"  # isaac_lab / mujoco / gazebo
  headless: false
  num_envs: 1
  
  scene:
    robot: "ur5e"
    gripper: "robotiq_2f85"
    camera: "realsense_d435i"
    objects:
      - type: "smt_0402"
        count: 20
      - type: "pcb_board"
        count: 5

perception:
  object_detection:
    model: "yolov8n"
    confidence_threshold: 0.5
    weights: "models/yolov8_3c.pt"
  
  pose_estimation:
    model: "foundationpose"
    weights: "models/foundationpose.pth"

planning:
  grasp_generation:
    model: "anygrasp"
    num_grasps: 100
    top_k: 10
  
  motion_planning:
    planner: "rrt_connect"
    planning_time: 5.0
    max_velocity_scaling: 0.5

control:
  joint_controller:
    type: "position"
    gains:
      kp: [100, 100, 100, 100, 100, 100]
      kd: [10, 10, 10, 10, 10, 10]

logging:
  level: "INFO"
  save_episodes: true
  output_dir: "logs/"
```

**加载配置：**

```python
import yaml

class SystemConfig:
    def __init__(self, config_file):
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
    
    def get(self, key_path):
        """获取嵌套配置值，如'simulation.platform'"""
        keys = key_path.split('.')
        value = self.config
        for key in keys:
            value = value[key]
        return value
    
    def set(self, key_path, new_value):
        """设置配置值"""
        keys = key_path.split('.')
        value = self.config
        for key in keys[:-1]:
            value = value[key]
        value[keys[-1]] = new_value

# 使用
config = SystemConfig('config/grasp_system.yaml')
platform = config.get('simulation.platform')
```

### 4.3 扩展示例

#### 4.3.1 添加新的物体检测算法

```python
from perception import PerceptionInterface

class YOLOv10Detector(PerceptionInterface):
    """使用YOLOv10的检测器"""
    
    def __init__(self, weights_path):
        from ultralytics import YOLO
        self.model = YOLO(weights_path)
    
    def detect_objects(self, rgb_image, depth_image):
        results = self.model(rgb_image)
        
        detected_objects = []
        for result in results[0].boxes:
            obj = DetectedObject(
                class_id=int(result.cls),
                confidence=float(result.conf),
                bbox=result.xyxy[0].tolist(),
            )
            detected_objects.append(obj)
        
        return detected_objects
    
    def estimate_pose(self, rgb_image, depth_image, object_mask):
        # 调用位姿估计算法
        return self.pose_estimator.estimate(rgb_image, depth_image, object_mask)

# 注册到系统
perception_registry = {
    'yolov8': YOLOv8Detector,
    'yolov10': YOLOv10Detector,
    'sam': SAMDetector,
}
```

#### 4.3.2 添加新的仿真平台

```python
from simulation import SimulationInterface

class PyBulletSimulation(SimulationInterface):
    """PyBullet仿真实现"""
    
    def __init__(self, config):
        import pybullet as p
        self.p = p
        self.client = p.connect(p.GUI if not config['headless'] else p.DIRECT)
        
        # 加载场景
        self.load_scene(config)
    
    def reset(self):
        self.p.resetSimulation()
        self.load_scene(self.config)
        return self.get_observation()
    
    def step(self, action):
        # 设置关节目标
        self.set_joint_targets(action)
        
        # 步进仿真
        self.p.stepSimulation()
        
        # 获取观测
        obs = self.get_observation()
        reward = self.compute_reward()
        done = self.check_done()
        
        return obs, reward, done, {}
    
    # ... 实现其他接口方法

# 注册
simulation_registry = {
    'isaac_lab': IsaacLabSimulation,
    'mujoco': MuJoCoSimulation,
    'gazebo': GazeboSimulation,
    'pybullet': PyBulletSimulation,
}
```

---

## 📊 第五部分：完整示例项目

### 5.1 快速启动项目

```bash
# 项目结构
3c_grasping_project/
├── config/
│   ├── grasp_system.yaml
│   └── robot_ur5e.yaml
├── models/
│   ├── yolov8_3c.pt
│   ├── anygrasp.tar
│   └── foundationpose.pth
├── assets/
│   ├── robots/
│   │   ├── ur5e.urdf
│   │   └── robotiq_2f85.urdf
│   └── objects/
│       ├── smt_0402.obj
│       └── pcb_board.obj
├── src/
│   ├── perception/
│   ├── planning/
│   ├── control/
│   └── simulation/
├── scripts/
│   ├── train.py
│   ├── evaluate.py
│   └── deploy.py
├── tests/
└── requirements.txt
```

**requirements.txt：**

```txt
# 仿真
isaaclab>=1.0.0
mujoco>=3.0.0
pybullet>=3.2.5

# 机器人
moveit-py>=2.5.0

# 感知
ultralytics>=8.0.0
open3d>=0.18.0
opencv-python>=4.8.0

# 规划
torch>=2.0.0
numpy>=1.24.0
trimesh>=4.0.0

# 强化学习
stable-baselines3>=2.0.0
gymnasium>=0.29.0

# 工具
pyyaml>=6.0
h5py>=3.9.0
matplotlib>=3.7.0
```

### 5.2 完整训练脚本

```python
# scripts/train.py
import argparse
from src.system import GraspingSystem
from src.simulation import SimulationFactory
from src.config import SystemConfig

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', default='config/grasp_system.yaml')
    parser.add_argument('--mode', choices=['train', 'eval', 'demo'], default='train')
    args = parser.parse_args()
    
    # 加载配置
    config = SystemConfig(args.config)
    
    # 创建仿真环境
    sim = SimulationFactory.create(config.get('simulation.platform'), config)
    
    # 创建抓取系统
    system = GraspingSystem(config, sim)
    
    if args.mode == 'train':
        print("开始训练...")
        system.train(num_episodes=10000)
    
    elif args.mode == 'eval':
        print("开始评估...")
        results = system.evaluate(num_episodes=100)
        print(f"成功率: {results['success_rate']*100:.1f}%")
    
    elif args.mode == 'demo':
        print("运行演示...")
        system.run_demo()

if __name__ == '__main__':
    main()
```

**运行命令：**

```bash
# 训练
python scripts/train.py --mode train

# 评估
python scripts/train.py --mode eval

# 演示
python scripts/train.py --mode demo
```

---

## 📚 第六部分：学习资源与参考

### 6.1 推荐学习路径

**初级（1-2个月）：**
1. 学习ROS 2基础
2. 熟悉一个仿真平台（Isaac Lab或MuJoCo）
3. 运行现成的抓取demo

**中级（2-3个月）：**
1. 深入学习点云处理（Open3D）
2. 训练目标检测模型（YOLOv8）
3. 使用GraspNet-1B数据集

**高级（3-6个月）：**
1. 实现自定义抓取算法
2. 强化学习训练
3. Sim2Real迁移

### 6.2 关键论文

| 论文 | 会议/期刊 | 代码 |
|------|----------|------|
| GraspNet-1Billion | CVPR 2020 | github.com/graspnet/graspnet-baseline |
| AnyGrasp | T-RO 2023 | graspnet.net/anygrasp |
| Contact-GraspNet | ICRA 2021 | github.com/NVlabs/contact_graspnet |
| FoundationPose | CVPR 2024 | github.com/NVlabs/FoundationPose |
| Isaac Gym | CoRL 2021 | isaac-gym.github.io |

### 6.3 开源项目参考

| 项目 | 描述 | 链接 |
|------|------|------|
| MoveIt 2 | ROS运动规划框架 | moveit.ros.org |
| Easy Manipulation Deployment | 端到端抓取系统 | github.com/ros-industrial/easy_manipulation_deployment |
| OpenManipulator-X | 开源机械臂 | github.com/ROBOTIS-GIT/open_manipulator |
| ROS Industrial | 工业机器人包 | github.com/ros-industrial |

### 6.4 社区资源

- **论坛：** ROS Discourse, Isaac Sim Forums
- **Slack：** Open Robotics Slack
- **YouTube：** The Construct, Articulated Robotics
- **课程：** Coursera "Modern Robotics", Udacity "Robotics ND"

---

## 🎯 附录：快速检查清单

### A. 开发环境检查

```bash
# 1. 检查CUDA
nvidia-smi

# 2. 检查ROS 2
ros2 --version

# 3. 检查Python包
python -c "import torch; print(torch.cuda.is_available())"
python -c "import open3d; print(open3d.__version__)"

# 4. 测试Isaac Lab（如使用）
cd IsaacLab
./isaaclab.sh -p source/standalone/tutorials/00_sim/create_empty.py
```

### B. 资源下载清单

- [ ] UR5/UR5e URDF模型
- [ ] Franka Panda URDF模型
- [ ] Robotiq夹爪模型
- [ ] YCB物体数据集（部分）
- [ ] GraspNet-1B数据集（注册）
- [ ] YOLOv8预训练权重
- [ ] AnyGrasp模型（需license）

### C. 性能目标

| 指标 | 研发阶段目标 | 生产阶段目标 |
|------|------------|------------|
| 仿真成功率 | ≥85% | ≥92% |
| 真实成功率 | ≥70% | ≥85% |
| 抓取周期 | ≤5s | ≤2s |
| 感知延迟 | ≤200ms | ≤100ms |
| 规划时间 | ≤1s | ≤500ms |

---

## 📧 技术支持

**文档更新：** 2025-10-09  
**版本：** v1.0  

**后续更新计划：**
- [ ] 添加柔性物体抓取专题
- [ ] 添加双臂协作案例
- [ ] 添加触觉传感器集成
- [ ] 添加云端训练方案

---

**🎉 现在您可以开始构建自己的机械臂抓取系统了！**

建议从以下步骤开始：
1. 安装Isaac Lab或MuJoCo（选一个）
2. 下载UR5 URDF模型
3. 运行第一个仿真demo
4. 逐步集成各个模块

祝研发顺利！🚀

